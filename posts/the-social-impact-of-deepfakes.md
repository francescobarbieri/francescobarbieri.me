---
title: 'The social impact of deepfakes'
date: '2023-06-28'
tag: 'Tech'
preview: 'Unraveling deepfakes: the alarming threat to truth, trust, and our digital reality.'
---

### Abstract / Introduction

Trust has become a first-order concept in AI, urging experts to call for measures ensuring it is "trustworthy". One of the dangers of AI are deepfakes, stunningly real videos that few would give a second glance to make sure they are real. They are perceived as a threat to democracies and online trust, through their potential to back sophisticated disinformation campaign. Moreover there are many social and psychological questions that deepfakes raised: how might deepfakes be used in social interaction? Are there strategies to debunking or countering deepfakes?

There has been ample work done in computer science on automatic generation and detection of deepfakes, indeed the number of deepfakes is increasingly exponentially. But to date there have only been a handful of social scientist who have examined the social impact of this technology.

![Graph representing the number of deepfakes from 2018 to 2020.](/articlesImg/the-social-impact-of-deepfakes/deepfakesGraph.svg)

### New tech, old concept

Deepfakes, as we know them today, started with the [Video Rewrite program](http://chris.bregler.com/videorewrite/), created in 1997 by Christoph Bregler, Michele Covell and Malcolm Stanley. The program altered existing video footage to create new content of someone mouthing words they didn't speak in the original version. This program was the first system to automate facial reanimation completely. 

As computer vision and artificial intelligence continued to advance, Ian Goodfellow's invention of [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GAN) in 2014 allowed the generation of much more realist fake  videos. GANs consists of two neural networks that compete to produce and discern high quality faked images. One is the “generator”, which creates images that loo like an original image, and the other is the “discriminator”, which tries to figure out if an image is fake or authentic. A feedback loop is generated between the two so that, as the generator learns, it gets better at fooling the discriminator by creating more realistic fakes and, as the discriminator learns, it finds more sophisticated ways of identifying the fakes.

This is the framework that generated many famous deepfakes, such as the [Obama video](https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo) which hits the headlines in 2018, or the more recent image of Pope Francis wearing a Balenciaga down jacked.

![Deepfake of Pope Francis wearing Balenciaga.](/articlesImg/the-social-impact-of-deepfakes/popeFrancisSwag.webp)

Now that we understand how deepfakes works at a high level, in this blog entry I want tackle a more fundamental question: can deepfakes make us doubt everything we see and hear?

### Insights from deception research

Despite numerous studies focusing on false memory acquisition and social influence through altered still images, the psychological processes and consequences of viewing AI-modified videos remain largely unstudied.

Deception lies at the heart of deepfakes, involving the intentional and purposeful act of misleading others. Studies on deception detection reveal that people struggle to accurately identify deception when evaluating messages, often acquiring false beliefs quite easily. Interestingly, the medium through which the message is conveyed — be it text, audio recordings, or videos — has minimal influence on detection accuracy. But not all mediums have the same impact. Deepfakes videos have the potential to have a greater impact because not only they change verbal content, but also the visual properties of how the message was conveyed, such as the movements of a person’s mouth saying something that he did not, or the behavior of a person doing something that he did not.

This increased impact is do to the nature of humans: is well established that we rely more on visual information than other forms of sensory information in most of the circumstances. This is known as the [Colavita effect](https://en.wikipedia.org/wiki/Colavita_visual_dominance_effect). Following the communication literature, people are also more likely to recall visual messages than verbal messages, and misleading visual information is more likely to generate false perception because of the “realism heuristic”, in which people are more likely to trust audiovisual modalities over verbal because the content has a higher resemblance to the real world. 

As we learn that video, once considered a medium immune to manipulation, becomes susceptible to “photoshopping” can we believe any media that we see? The philosopher Don Fallis refers to this as the *epistemic threat of deepfakes*. This threat challenges our ability to acquire knowledge through media consumption and, because of the dominance of the visual system, videos has hight information carrying potential. But as deepfakes proliferate people became aware that videos can be fakes, therefore the amount of information that videos carry is diminished. According to Fallis, deepfakes can undermine our shared understanding of the world, and the role that journalism and other media have.

### Will deepfakes succeed in deception?

Unfortunately, a notable empirical study on deepfakes provide some evidence that they may succeed. Examining the impact of deepfakes on trust in news, the study discovered that while individuals were unlikely to be entirely deceived by a deepfake, exposure to such manipulated content increased their overall skepticism over the media. In line with Fallis’s conceptualization of epistemic threat, this increased sense of uncertainty ultimately led participants to diminish their trust in news, aligning with the worst expectations.

Despite the pessimistic outlook on the implications of deepfake technology, it is crucial to consider the active role of media consumers. The analyzed perspective assumes a rather passive stance towards media consumption. However, it is worth noting that throughout history humans have adapted to various forms of deception. Initially people tend to place trust in one another until they encounter reasons to become suspicious or more cautious, a state called as trust default. This trust default can be disrupted when individuals come across inconsistent information, receive warnings from third parties, or acquire knowledge about new deceptive techniques. An example is email spam, which has become less effective over time due in part to people’s awareness of its existence. In the same way, it is possible for people to develop resilience to novel forms of deception such as deepfakes.

An important harm that it’s not considered in this article is the portrayal of non-consenting victims in deepfakes, depicting them engaging in actions or making statements they never did. Among the earliest, and most prevalent, forms of deepfakes are those that manipulate pornography showing individuals involved in nonconsensual sexual acts placing their faces on another person’s body. Considering the profound influence of visual stimuli on shaping our beliefs, as previously discussed, and the potential impact on an individual’s self-identity, the consequences for the victim can be devastating. It is not challenging to imagine how deepfakes could be used for purposes of extortion, humiliation or harassment.

### Final thoughts

The malicious use of deepfakes could be mitigated through technical and legal measures, such as ensuring they are properly labelled as non-authentic. The European Parliament has called in 2021 for a mandatory labelling and other rules to flag and remove illegal content. But such measures cannot respond to all risks of malicious deepfakes.

Even though deepfake technology has the potential to undermine our trust in media or manipulate our perception of reality, it could also become a common tool as people use it to improve their day-to-day communication and activities. Deepfake technology has the potential to change the multimedia, and content creation industries. This is due to the fact that through deepfakes, dialogues or expressions could synthetically be replaced, reducing the cost of production and saving time enabling multilingual dubbing, and enhancing voice synchronization in gaming and virtual reality applications.

From what I’ve been writing so far, what can be confidently affirmed is that there are many important psychological, social, and ethical issues that require innovative and careful empirical analyses of the social impact of deepfake technologies.